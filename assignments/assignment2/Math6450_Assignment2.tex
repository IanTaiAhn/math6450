\documentclass[8pt, twocolumn]{extarticle}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    % UNICODE CHARACTER SUPPORT - FIX FOR MISSING CHARACTERS
    \usepackage{newunicodechar}
    \usepackage{textcomp}  % For additional symbols
    \usepackage{amssymb}   % For mathematical symbols

    % Your existing definitions
    \newunicodechar{Ï}{\ensuremath{\rho}}
    \newunicodechar{â‰ˆ}{\ensuremath{\approx}}
    \newunicodechar{â‰¤}{\ensuremath{\leq}}
    \newunicodechar{â‰¥}{\ensuremath{\geq}}
    \newunicodechar{Ã—}{\ensuremath{\times}}
    \newunicodechar{Î²}{\ensuremath{\beta}}
    \newunicodechar{Î±}{\ensuremath{\alpha}}
    \newunicodechar{Â²}{\ensuremath{^2}}
    \newunicodechar{â‚€}{\ensuremath{_0}}
    \newunicodechar{â‚}{\ensuremath{_1}}
    \newunicodechar{â‚‚}{\ensuremath{_2}}
    \newunicodechar{â‰ }{\ensuremath{\neq}}
    \newunicodechar{âœ“}{\ensuremath{\checkmark}}
    
    % Additional subscripts from your error messages
    \newunicodechar{â‚ƒ}{\ensuremath{_3}}
    \newunicodechar{â‚„}{\ensuremath{_4}}
    \newunicodechar{â‚…}{\ensuremath{_5}}
    \newunicodechar{â‚†}{\ensuremath{_6}}
    \newunicodechar{â‚‡}{\ensuremath{_7}}
    \newunicodechar{áµ¢}{\ensuremath{_i}}

    % Greek letters
    \newunicodechar{Îµ}{\ensuremath{\varepsilon}}

    % Mathematical symbols
    \newunicodechar{âˆ‚}{\ensuremath{\partial}}
    \newunicodechar{âˆ¼}{\ensuremath{\sim}}

    % % Special symbols (these might need different approaches)
    % \newunicodechar{ðŸŽ¯}{\textbullet}  % Fallback to bullet
    % \newunicodechar{âš }{\textwarning} % Warning symbol (requires textcomp)
    % END UNICODE CHARACTER SUPPORT


    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \makeatletter
    \newsavebox\pandoc@box
    \newcommand*\pandocbounded[1]{%
      \sbox\pandoc@box{#1}%
      % scaling factors for width and height
      \Gscale@div\@tempa\textheight{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
      \Gscale@div\@tempb\linewidth{\wd\pandoc@box}%
      % select the smaller of both
      \ifdim\@tempb\p@<\@tempa\p@
        \let\@tempa\@tempb
      \fi
      % scaling accordingly (\@tempa < 1)
      \ifdim\@tempa\p@<\p@
        \scalebox{\@tempa}{\usebox\pandoc@box}%
      % scaling not needed, use as it is
      \else
        \usebox{\pandoc@box}%
      \fi
    }
    \makeatother

    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Math6450\_Assignment2}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    \maketitle
    \textbf{1 Data Exploration}
    \begin{Verbatim}[commandchars=\\\{\}]
(a) Descriptive Statistics for Continuous Variables
--------------------------------------------------

Comprehensive Descriptive Statistics:
               Mean   Median  Std Dev  Minimum  Maximum  Skewness  Kurtosis
claims       18.049   17.845    6.448     0.72    41.39     0.254     0.095
deductible    2.490    1.905    1.942     0.51    10.00     1.542     2.351
coverage    189.014  186.750   72.169    50.00   424.50     0.145    -0.292
age          15.438   11.000   14.227     1.00    85.00     1.869     4.496
premium       2.969    2.945    0.822     0.50     5.78     0.245     0.030
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Correlation Matrix for Continuous Variables
--------------------------------------------------

Correlation Matrix:
            claims  deductible  coverage    age  premium
claims       1.000      -0.265     0.761  0.199    0.793
deductible  -0.265       1.000    -0.066  0.006   -0.059
coverage     0.761      -0.066     1.000 -0.015    0.723
age          0.199       0.006    -0.015  1.000    0.314
premium      0.793      -0.059     0.723  0.314    1.000

Variable with strongest linear relationship with 'claims':
Variable: premium
Correlation coefficient: 0.793
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Skewness Analysis and Log Transformation Assessment
--------------------------------------------------

Skewness Assessment:
Rule of thumb: |skewness| > 1 indicates highly skewed distribution
Rule of thumb: 0.5 < |skewness| < 1 indicates moderately skewed distribution

claims:
  Skewness: 0.254
  Assessment: Approximately symmetric

deductible:
  Skewness: 1.542
  Assessment: Highly skewed
  Log transformation skewness: 0.134
  Improvement from log transformation: 1.408
  Recommendation: Log transformation would improve normality

coverage:
  Skewness: 0.145
  Assessment: Approximately symmetric

age:
  Skewness: 1.869
  Assessment: Highly skewed
  Log transformation skewness: -0.347
  Improvement from log transformation: 1.523
  Recommendation: Log transformation would improve normality

premium:
  Skewness: 0.245
  Assessment: Approximately symmetric
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_7_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Summary of Findings:
--------------------------------------------------
Variables with skewed distributions: deductible, age
Variable most strongly correlated with claims: premium (r = 0.793)

Data Overview:
Total observations: 1,340
Variables analyzed: 5
Missing values: 0
    \end{Verbatim}
    \textbf{2 Simple Linear Regression}
    \begin{Verbatim}[commandchars=\\\{\}]
Dataset Information:
Total observations: 1,340
Observations used in regression: 1,340
Missing values removed: 0

(a) Simple Linear Regression Model Fitting
--------------------------------------------------
Model Coefficients:
Intercept (Î²â‚€): 5.2054
Slope (Î²â‚): 0.0679

Fitted Regression Equation:
Claims = 5.2054 + 0.0679 Ã— Coverage

In mathematical notation:
Å· = 5.2054 + 0.0679x
where Å· = predicted claims, x = coverage
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Interpretation of Slope Coefficient
--------------------------------------------------
Slope coefficient: 0.0679

Practical Interpretation:
â€¢ For every 1-unit increase in coverage, claims are expected to increase by
0.0679 units, on average.
â€¢ This indicates a positive relationship between coverage and claims.
â€¢ Properties with higher coverage amounts tend to have higher claims.

Alternative interpretation:
â€¢ For every 100-unit increase in coverage, claims change by 6.79 units, on
average.

Example predictions:
â€¢ Coverage = 100: Predicted Claims = 12.00
â€¢ Coverage = 150: Predicted Claims = 15.40
â€¢ Coverage = 200: Predicted Claims = 18.80
â€¢ Coverage = 250: Predicted Claims = 22.19
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Coefficient of Determination (RÂ²) Analysis
--------------------------------------------------
Model Performance Metrics:
RÂ² (Coefficient of Determination): 0.5784
RÂ² as percentage: 57.84\%
Correlation coefficient (r): 0.7605
Root Mean Square Error (RMSE): 4.1850

Interpretation of RÂ²:
â€¢ 57.84\% of the variation in claims is explained by coverage.
â€¢ 42.16\% of the variation in claims is due to other factors not included in the
model.
â€¢ The linear relationship between coverage and claims is moderate (RÂ² = 0.5784).

Statistical Significance:
â€¢ t-statistic: 42.8442
â€¢ p-value: 0.0000
â€¢ Degrees of freedom: 1338
â€¢ The relationship is statistically significant at the 5\% level.
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Summary Table:
         Metric  Value                                 Interpretation
 Intercept (Î²â‚€) 5.2054              Expected claims when coverage = 0
     Slope (Î²â‚) 0.0679 Change in claims per unit increase in coverage
             RÂ² 0.5784                    57.8\% of variance explained
Correlation (r) 0.7605                    Linear association strength
           RMSE 4.1850                       Average prediction error
   Observations   1340                                    Sample size
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
Key Findings Summary:
â€¢ Regression equation: Claims = 5.2054 + 0.0679 Ã— Coverage
â€¢ Slope interpretation: Each additional unit of coverage is associated with a
0.0679 unit change in claims
â€¢ Model explains 57.8\% of the variation in claims
â€¢ The relationship is statistically significant (p = 0.0000)
    \end{Verbatim}
    \testbf{3 Multiple Regression Model}
    \begin{Verbatim}[commandchars=\\\{\}]
Dependent Variable: claims
Explanatory Variables: deductible, coverage, age, prior\_claims, premium

Dataset Information:
Total observations: 1,340
Complete cases used: 1,340
Observations removed (missing data): 0
Number of explanatory variables: 5

(a) Fitted Regression Equation
--------------------------------------------------
Coefficient Estimates (rounded to 3 decimal places):
Intercept (Î²â‚€): 3.208
Î²\_1 (deductible): -0.728
Î²\_2 (coverage): 0.062
Î²\_3 (age): 0.091
Î²\_4 (prior\_claims): 2.580
Î²\_5 (premium): 0.495

Fitted Regression Equation:
Claims = 3.208 - 0.728 Ã— deductible + 0.062 Ã— coverage + 0.091 Ã— age + 2.580 Ã—
prior\_claims + 0.495 Ã— premium

Compact Mathematical Form:
Å· = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + Î²â‚ƒxâ‚ƒ + Î²â‚„xâ‚„ + Î²â‚…xâ‚…
Å· = 3.208 + -0.728xâ‚ + 0.062xâ‚‚ + 0.091xâ‚ƒ + 2.580xâ‚„ + 0.495xâ‚…
where xâ‚=deductible, xâ‚‚=coverage, xâ‚ƒ=age, xâ‚„=prior\_claims, xâ‚…=premium
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Standard Errors for Each Coefficient
--------------------------------------------------
Standard Errors:
Intercept (Î²â‚€): 0.3172
Î²\_1 (deductible): 0.0394
Î²\_2 (coverage): 0.0020
Î²\_3 (age): 0.0068
Î²\_4 (prior\_claims): 0.1210
Î²\_5 (premium): 0.2118

Additional Statistics (t-statistics and p-values):
Coefficient     Estimate   Std Error    t-stat     p-value    Significance
--------------------------------------------------
Intercept       3.208      0.3172       10.113     0.0000     ***
deductible      -0.728     0.0394       -18.459    0.0000     ***
coverage        0.062      0.0020       30.624     0.0000     ***
age             0.091      0.0068       13.401     0.0000     ***
prior\_claims    2.580      0.1210       21.316     0.0000     ***
premium         0.495      0.2118       2.338      0.0195     *
Significance codes: *** p<0.001, ** p<0.01, * p<0.05
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Model Performance Statistics
--------------------------------------------------
RÂ² (Coefficient of Determination): 0.8130
Adjusted RÂ²: 0.8123
Residual Standard Deviation: 2.7938

Additional Model Statistics:
Multiple R (Correlation): 0.9016
Residual Sum of Squares (RSS): 10412.1409
Mean Squared Error (MSE): 7.8052
F-statistic: 1159.6202
F-statistic p-value: 0.000000
Overall model significance: Yes (Î± = 0.05)

Degrees of Freedom:
Model: 5
Residual: 1334
Total: 1339

Summary Results Table:
       Variable  Coefficient  Std\_Error  Coefficient\_Rounded
0     Intercept       3.2078     0.3172                3.208
1    deductible      -0.7278     0.0394               -0.728
2      coverage       0.0621     0.0020                0.062
3           age       0.0906     0.0068                0.091
4  prior\_claims       2.5797     0.1210                2.580
5       premium       0.4953     0.2118                0.495

Model Performance Table:
             Statistic     Value
                    RÂ²    0.8130
           Adjusted RÂ²    0.8123
Residual Std Deviation    2.7938
           F-statistic 1159.6202
      p-value (F-test)  0.000000
          Observations      1340
             Variables         5
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Key Results Summary:
âœ“ Multiple regression equation fitted with 5 explanatory variables
âœ“ Model explains 81.3\% of variance in claims (RÂ² = 0.8130)
âœ“ Adjusted RÂ² = 0.8123 (accounts for number of variables)
âœ“ Residual standard deviation = 2.7938
âœ“ Overall model is significant (F-test p-value = 0.000000)
âœ“ Standard errors calculated for all 6 coefficients
    \end{Verbatim}
    \textbf{4 Statistical Inference}
    \begin{Verbatim}[commandchars=\\\{\}]
Multiple Linear Regression Model: Claims vs (Deductible, Coverage, Age,
Prior\_Claims, Premium)
Model Summary:
Observations: 1340
Variables: 5
Degrees of freedom (residual): 1334
RÂ²: 0.8130
MSE: 7.8052

Coefficient Estimates:
Variable        Coefficient  Std Error    t-statistic  p-value
--------------------------------------------------
deductible      -0.7278      0.0394       -18.4591     0.0000
coverage        0.0621       0.0020       30.6239      0.0000
age             0.0906       0.0068       13.4010      0.0000
prior\_claims    2.5797       0.1210       21.3156      0.0000
premium         0.4953       0.2118       2.3382       0.0195

(a) Testing Significance of Age Coefficient
Hypothesis Test for Age Coefficient:

Null Hypothesis (Hâ‚€): Î²\_age = 0
Alternative Hypothesis (Hâ‚): Î²\_age â‰  0
Significance level (Î±): 0.05
Test type: Two-tailed t-test

Test Statistics:
Age coefficient (Î²\_age): 0.0906
Standard error (SE): 0.0068
t-statistic: 13.4010
Degrees of freedom: 1334
p-value: 0.0000
Critical value (Â±): 1.9617

Decision Rule:
Reject Hâ‚€ if |t-statistic| > 1.9617 OR if p-value < 0.05

Conclusion:
âœ“ REJECT Hâ‚€: The coefficient for age IS statistically significant at the 5\%
level.
  |t-statistic| = 13.4010 > 1.9617
  p-value = 0.0000 < 0.05
  Age has a statistically significant effect on claims.
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) 95\% Confidence Interval for Prior Claims Coefficient
Confidence Interval Calculation:
Coefficient (Î²\_prior\_claims): 2.5797
Standard error: 0.1210
Degrees of freedom: 1334
Confidence level: 95\%

Confidence Interval Formula:
CI = Î²Ì‚ Â± t\_(Î±/2,df) Ã— SE(Î²Ì‚)
CI = 2.5797 Â± 1.9617 Ã— 0.1210
CI = 2.5797 Â± 0.2374

95\% Confidence Interval for Prior Claims Coefficient:
[2.3423, 2.8171]

Practical Interpretation:
â€¢ We are 95\% confident that the true effect of having prior claims on current
claims
  is between 2.3423 and 2.8171 units.
â€¢ Since the entire interval is positive, prior claims consistently INCREASE
current claims.
â€¢ Properties with prior claims have significantly higher current claims than
those without.
â€¢ The width of the interval (0.4748) indicates the precision of our estimate.
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Overall F-test for Model Significance
Overall F-test for Regression Model:

Null Hypothesis (Hâ‚€): Î²â‚ = Î²â‚‚ = Î²â‚ƒ = Î²â‚„ = Î²â‚… = 0
  (All explanatory variables have no effect on claims)
Alternative Hypothesis (Hâ‚): At least one Î²áµ¢ â‰  0
  (At least one explanatory variable has a significant effect)
Significance level (Î±): 0.05

Test Statistics:
Total Sum of Squares (TSS): 55667.4953
Explained Sum of Squares (ESS): 45255.3543
Residual Sum of Squares (RSS): 10412.1409
Mean Square Regression (MSR): 9051.0709
Mean Square Error (MSE): 7.8052

F-statistic: 1159.6202
Degrees of freedom: (5, 1334)
p-value: 0.000000
Critical F-value (Î± = 0.05): 2.2208

Decision Rule:
Reject Hâ‚€ if F-statistic > 2.2208 OR if p-value < 0.05

Conclusion:
âœ“ REJECT Hâ‚€: The regression model IS statistically significant at the 5\% level.
  F-statistic = 1159.6202 > 2.2208
  p-value = 0.000000 < 0.05
  At least one explanatory variable has a significant effect on claims.
  The model explains a significant portion of the variation in claims.

Model Performance Context:
RÂ² = 0.8130 (81.3\% of variance explained)
The model performs well in predicting claims.
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Summary of All Statistical Tests:
                    Test             Statistic  p-value         Conclusion
Age Coefficient (t-test)           t = 13.4010   0.0000        Significant
         Prior Claims CI CI = [2.3423, 2.8171]      N/A Does not contain 0
  Overall Model (F-test)         F = 1159.6202 0.000000  Model Significant

LaTeX Summary Table:
\textbackslash{}begin\{table\}
\textbackslash{}caption\{Summary of Statistical Tests\}
\textbackslash{}label\{tab:hypothesis\_tests\}
\textbackslash{}begin\{tabular\}\{llll\}
\textbackslash{}toprule
Test \& Statistic \& p-value \& Conclusion \textbackslash{}\textbackslash{}
\textbackslash{}midrule
Age Coefficient (t-test) \& t = 13.4010 \& 0.0000 \& Significant \textbackslash{}\textbackslash{}
Prior Claims CI \& CI = [2.3423, 2.8171] \& N/A \& Does not contain 0 \textbackslash{}\textbackslash{}
Overall Model (F-test) \& F = 1159.6202 \& 0.000000 \& Model Significant \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
\textbackslash{}end\{table\}
    \end{Verbatim}
    \textbf{5 Binary Variables and Model Interpretation}
    \begin{Verbatim}[commandchars=\\\{\}]
Adding 'type' and 'location' to the original model
Dependent Variable: claims
Original Variables: deductible, coverage, age, prior\_claims, premium
New Variables: type, location

Data Summary:
Original model observations: 1,340
Extended model observations: 1,340

Extended Model Summary:
Observations: 1340
Variables: 7
RÂ²: 0.8263
Adjusted RÂ²: 0.8254
Residual Standard Error: 2.6939

(a) Extended Regression Model Equation
Coefficient Estimates:
Variable        Coefficient  Std Error    t-stat     p-value
--------------------------------------------------
Intercept       3.027        0.3171
deductible      -0.713       0.0381       -18.706    0.0000
coverage        0.058        0.0022       26.539     0.0000
age             0.077        0.0070       10.935     0.0000
prior\_claims    2.392        0.1254       19.077     0.0000
premium         1.019        0.2378       4.284      0.0000
type            -1.419       0.1699       -8.355     0.0000
location        0.859        0.1731       4.959      0.0000

Fitted Regression Equation:
Claims = 3.027 - 0.713 Ã— deductible + 0.058 Ã— coverage + 0.077 Ã— age + 2.392 Ã—
prior\_claims + 1.019 Ã— premium - 1.419 Ã— type + 0.859 Ã— location

Detailed Mathematical Form:
Claims = 3.027 + -0.713Ã—deductible + 0.058Ã—coverage
         + 0.077Ã—age + 2.392Ã—prior\_claims + 1.019Ã—premium
         + -1.419Ã—type + 0.859Ã—location
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Interpretation of Type Coefficient
Type Coefficient Analysis:
Coefficient (Î²\_type): -1.419
Standard Error: 0.1699
t-statistic: -8.355
p-value: 0.0000

Type variable coding: [np.int64(0), np.int64(1)]

Practical Interpretation:
â€¢ Properties with type = 1 have claims that are 1.419 units LOWER than
properties with type = 0,
  holding all other variables constant.

Assuming standard coding (0 = Commercial, 1 = Residential):
â€¢ Residential properties have claims that are 1.419 units lower than commercial
properties.
â€¢ This suggests commercial properties are associated with higher insurance
claims.

Statistical Significance:
â€¢ The type coefficient IS statistically significant (p = 0.0000 < 0.05)
â€¢ We can be confident that property type has a real effect on claims.
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Partial F-test for Model Improvement
Model Comparison (same sample size: 1340):
Model                RÂ²           Adj RÂ²       Variables  RSS
--------------------------------------------------
Original             0.8130       0.8123       5          10412.1409
Extended             0.8263       0.8254       7          9666.7444

RÂ² Improvement: 0.0134 (1.34 percentage points)

Partial F-test:
Hâ‚€: Î²\_type = Î²\_location = 0 (binary variables add no explanatory power)
Hâ‚: At least one of Î²\_type or Î²\_location â‰  0 (binary variables improve the
model)

Partial F-test Calculations:
RSS(original): 10412.1409
RSS(extended): 9666.7444
Reduction in RSS: 745.3965
Additional variables (q): 2
DF residual (extended): 1332

F-statistic: 51.3548
Degrees of freedom: (2, 1332)
p-value: 0.0000
Critical F-value (Î± = 0.05): 3.0025

Conclusion:
âœ“ REJECT Hâ‚€: Adding type and location SIGNIFICANTLY improves the model
  F = 51.3548 > 3.0025
  p-value = 0.0000 < 0.05
  The binary variables provide significant additional explanatory power.

Model Improvement Assessment:
â€¢ RÂ² improved by 0.0134 (1.34 percentage points) - this is modest
â€¢ Extended model explains 82.6\% vs 81.3\% of variance
â€¢ Adjusted RÂ² increased from 0.8123 to 0.8254
â€¢ The improvement in adjusted RÂ² suggests the added variables are worthwhile
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_39_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Executive Summary:
                  Aspect                                             Finding
 Extended Model Equation Claims = 3.027 + {\ldots} + -1.419Ã—type + 0.859Ã—location
        Type Coefficient                                              -1.419
             Type Effect                       Type=1 has 1.419 lower claims
Statistical Significance                            Significant (p = 0.0000)
          RÂ² Improvement                     0.0134 (1.34 percentage points)
   Partial F-test Result                Significant improvement (p = 0.0000)
    \end{Verbatim}
    \textbf{6 Interaction Effects}
    \begin{Verbatim}[commandchars=\\\{\}]
Regression Model with Interaction Term: Deductible Ã— Type
Model Features: deductible, type, coverage, age, prior\_claims, premium
Interaction Term: deductible Ã— type

Data Summary:
Total observations: 1,340
Complete cases used: 1,340
Missing values removed: 0
Type variable coding: [np.int64(0), np.int64(1)]

Interaction Term (deductible Ã— type) Statistics:
Mean: 1.5335
Std Dev: 1.9042
Range: [0.0000, 10.0000]

Model Summary:
RÂ²: 0.8233
Adjusted RÂ²: 0.8224
Residual Standard Error: 2.7172
F-statistic: 886.8341

Coefficient Estimates:
Variable           Coefficient  Std Error    t-stat     p-value    Sig
--------------------------------------------------
Intercept          3.2856       0.3300
deductible         -0.6729      0.0596       -11.2894   0.0000     ***
type               -1.2573      0.2598       -4.8392    0.0000     ***
coverage           0.0553       0.0021       25.9580    0.0000     ***
age                0.0703       0.0070       10.1034    0.0000     ***
prior\_claims       2.2568       0.1234       18.2905    0.0000     ***
premium            1.3647       0.2290       5.9595     0.0000     ***
deductible\_x\_type  -0.0946      0.0779       -1.2151    0.2245
Significance codes: *** p<0.001, ** p<0.01, * p<0.05

(a) Regression Function with Interaction Term
General Form:
Claims = Î²â‚€ + Î²â‚Ã—deductible + Î²â‚‚Ã—type + Î²â‚ƒÃ—coverage + Î²â‚„Ã—age + Î²â‚…Ã—prior\_claims +
Î²â‚†Ã—premium + Î²â‚‡Ã—(deductibleÃ—type) + Îµ

Fitted Regression Equation:
Claims = 3.2856 - 0.6729Ã—deductible - 1.2573Ã—type + 0.0553Ã—coverage + 0.0703Ã—age
+ 2.2568Ã—prior\_claims + 1.3647Ã—premium - 0.0946Ã—(deductibleÃ—type)

With Coefficient Values:
Claims = 3.2856 + -0.6729Ã—deductible + -1.2573Ã—type
         + 0.0553Ã—coverage + 0.0703Ã—age + 2.2568Ã—prior\_claims
         + 1.3647Ã—premium + -0.0946Ã—(deductibleÃ—type)
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Interpretation of Deductible Effect by Property Type
Key Coefficients:
Î²â‚ (deductible): -0.6729
Î²â‚‚ (type): -1.2573
Î²â‚‡ (deductibleÃ—type): -0.0946

Interpretation of Interaction Effect:
The interaction model allows the effect of deductible to differ by property
type.

For Commercial Properties (type = 0):
âˆ‚Claims/âˆ‚deductible = Î²â‚ + Î²â‚‡Ã—0 = Î²â‚ = -0.6729
â€¢ A 1-unit increase in deductible changes claims by -0.6729 units for commercial
properties.

For Residential Properties (type = 1):
âˆ‚Claims/âˆ‚deductible = Î²â‚ + Î²â‚‡Ã—1 = Î²â‚ + Î²â‚‡ = -0.6729 + -0.0946 = -0.7675
â€¢ A 1-unit increase in deductible changes claims by -0.7675 units for
residential properties.

Comparison:
Difference in deductible effect: -0.0946
â€¢ The deductible effect is 0.0946 units MORE NEGATIVE for residential
properties.
â€¢ Deductible increases have a stronger negative effect on residential claims
than commercial claims.

Practical Business Interpretation:
â€¢ Higher deductibles are associated with lower claims for both property types
â€¢ This association is STRONGER for residential properties
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Statistical Significance Test for Interaction Term
Hypothesis Test for Interaction Term:
Hâ‚€: Î²â‚‡ = 0 (no interaction between deductible and type)
Hâ‚: Î²â‚‡ â‰  0 (significant interaction exists)
Significance level: Î± = 0.05

Test Statistics:
Interaction coefficient (Î²â‚‡): -0.0946
Standard error: 0.0779
t-statistic: -1.2151
Degrees of freedom: 1332
p-value: 0.2245
Critical value (Â±): 1.9617

Decision Rule:
Reject Hâ‚€ if |t-statistic| > 1.9617 OR if p-value < 0.05

Conclusion:
FAIL TO REJECT Hâ‚€: The interaction term is NOT statistically significant at the
5\% level.
  |t-statistic| = 1.2151 â‰¤ 1.9617
  p-value = 0.2245 â‰¥ 0.05
  The effect of deductible on claims does NOT differ significantly between
property types.
  The interaction term may not be necessary.

95\% Confidence Interval for Interaction Coefficient:
[-0.2473, 0.0581]
â€¢ The interval contains zero - the direction of the interaction effect is
uncertain
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_47_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Executive Summary:
                  Aspect
Result
     Model Specification Claims \textasciitilde{} deductible + type + coverage + age +
prior\_claims + premium + deductibleÃ—type
 Interaction Coefficient
-0.0946 (SE = 0.0779)
       Commercial Effect
-0.6729 per unit deductible
      Residential Effect
-0.7675 per unit deductible
              Difference
-0.0946
Statistical Significance
Not significant (p = 0.2245)
                Model RÂ²
0.8233

Model Interpretation:
â€¢ The non-significant interaction suggests that deductible effects are
  similar across commercial and residential properties
â€¢ A simpler model without interaction may be adequate
    \end{Verbatim}
    \textbf{7 Residual Analysis}
    \begin{Verbatim}[commandchars=\\\{\}]
Extended Multiple Linear Regression Model
Variables: deductible, coverage, age, prior\_claims, premium, type, location
Model Summary:
Observations: 1,340
Variables: 7
RÂ²: 0.8263
Residual Standard Error: 2.6939

(a) Residuals vs Fitted Values Analysis
Residuals vs Fitted Values Analysis:
Residual range: [-3.376, 15.203]
Fitted values range: [0.792, 39.985]

Pattern Analysis:
Correlation between fitted values and squared residuals: 0.0310
â€¢ Variance appears roughly constant
â€¢ Correlation magnitude suggests homoscedasticity (constant variance)

Linearity Assessment:
Mean residuals by fitted value terciles:
â€¢ Low tercile: -0.0800
â€¢ Middle tercile: 0.0229
â€¢ High tercile: 0.0572
â€¢ Maximum deviation from zero: 0.0800 (suggests linear relationship is
appropriate)
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Q-Q Plot and Normality Analysis
Normality Test Results:
Shapiro-Wilk Test:
  Statistic: 0.8106
  p-value: 0.0000
  REJECT normality at Î±=0.05

Jarque-Bera Test:
  Statistic: 2188.1490
  p-value: 0.0000
  REJECT normality at Î±=0.05

Kolmogorov-Smirnov Test:
  Statistic: 0.1468
  p-value: 0.0000
  REJECT normality at Î±=0.05

Descriptive Statistics for Normality:
Skewness: 1.9531 (Normal â‰ˆ 0)
Kurtosis: 4.8921 (Normal â‰ˆ 0)
Skewness interpretation: highly skewed
Kurtosis interpretation: heavy-tailed

Overall Normality Assessment: Assumption appears to be violated
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(c) Outliers and Influential Points Analysis
Diagnostic Thresholds:
Outlier threshold (standardized residuals): Â±3
High leverage threshold: 0.0119
High Cook's distance threshold: 0.0030

Outliers and Influential Points:
Observations with |standardized residuals| > 3: 31
Observations with |studentized residuals| > 3: 31
High leverage points: 73
High Cook's distance points: 74

Most Extreme Observations:
Highest Residual: Observation 315
  Fitted value: 23.547
  Actual value: 38.750
  Standardized residual: 5.643
  Leverage: 0.0072
  Cook's distance: 0.0331
Highest Leverage: Observation 262
  Fitted value: 34.070
  Actual value: 36.160
  Standardized residual: 0.776
  Leverage: 0.0305
  Cook's distance: 0.0027
Highest Cooks: Observation 315
  Fitted value: 23.547
  Actual value: 38.750
  Standardized residual: 5.643
  Leverage: 0.0072
  Cook's distance: 0.0331
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
<Figure size 640x480 with 0 Axes>
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
Detailed Analysis of Problematic Observations:
--------------------------------------------------
 Obs Fitted Actual Std\_Residual Leverage Cooks\_D                 Issues
   1 13.477 22.670        3.412   0.0032  0.0054 Outlier, High Cook's D
   2  5.711  3.340       -0.880   0.0122  0.0014          High Leverage
  14 20.959 20.000       -0.356   0.0128  0.0002          High Leverage
  36 10.929  8.700       -0.827   0.0142  0.0014          High Leverage
  70 13.967 11.670       -0.852   0.0130  0.0014          High Leverage
  71 20.337 24.990        1.727   0.0074  0.0032          High Cook's D
  73 30.965 29.670       -0.481   0.0141  0.0005          High Leverage
 118 22.728 22.290       -0.163   0.0193  0.0001          High Leverage
 122  5.247 10.110        1.805   0.0072  0.0034          High Cook's D
 129 31.861 36.730        1.807   0.0092  0.0043          High Cook's D

{\ldots} and 124 more observations with issues.


Diagnostic Summary:
1. Linearity: suggests linear relationship is appropriate
2. Homoscedasticity: suggests homoscedasticity (constant variance)
3. Normality: Assumption appears to be violated
4. Outliers: 31 potential outliers identified
5. Influential Points: 74 high Cook's distance observations

Recommendations:
â€¢ Consider transformation of variables or robust regression methods
â€¢ Examine influential points - consider their impact on coefficient estimates
    \end{Verbatim}
    \textbf{8 Model Comparison and Selection}
    \begin{Verbatim}[commandchars=\\\{\}]
Comparing three different model specifications:
Model A: claims \textasciitilde{} deductible + coverage + age + prior\_claims + premium
Model B: claims \textasciitilde{} deductible + coverage + age + prior\_claims + premium + type +
location
Model C: claims \textasciitilde{} deductible + coverage + prior\_claims + premium + type

Data Summary:
Original dataset size: 1,340
Complete cases for all models: 1,340
Cases removed due to missing data: 0

-------------------- Model A --------------------
Variables: deductible, coverage, age, prior\_claims, premium
Number of variables: 5
RÂ²: 0.8130
Adjusted RÂ²: 0.8123
Residual Standard Deviation: 2.7938
AIC: 6566.17
BIC: 6592.18
Significant coefficients (p < 0.05): 5/5

-------------------- Model B --------------------
Variables: deductible, coverage, age, prior\_claims, premium, type, location
Number of variables: 7
RÂ²: 0.8263
Adjusted RÂ²: 0.8254
Residual Standard Deviation: 2.6939
AIC: 6472.65
BIC: 6509.05
Significant coefficients (p < 0.05): 7/7

-------------------- Model C --------------------
Variables: deductible, coverage, prior\_claims, premium, type
Number of variables: 5
RÂ²: 0.8095
Adjusted RÂ²: 0.8088
Residual Standard Deviation: 2.8197
AIC: 6590.93
BIC: 6616.94
Significant coefficients (p < 0.05): 5/5


(a) Model Comparison Table
Primary Comparison Metrics:
  Model Variables     RÂ²  Adj\_RÂ²  Residual\_SD
Model A    5 vars 0.8130  0.8123       2.7938
Model B    7 vars 0.8263  0.8254       2.6939
Model C    5 vars 0.8095  0.8088       2.8197

Additional Model Selection Criteria:
  Model     AIC     BIC  F\_statistic Sig\_Coefs
Model A 6566.17 6592.18      1159.62       5/5
Model B 6472.65 6509.05       905.51       7/7
Model C 6590.93 6616.94      1133.51       5/5

Best Model by Criterion:
â€¢ Highest RÂ²: Model B (0.8263)
â€¢ Highest Adjusted RÂ²: Model B (0.8254)
â€¢ Lowest Residual SD: Model B (2.6939)
â€¢ Lowest AIC: Model B (6472.65)
â€¢ Lowest BIC: Model B (6509.05)

Model Complexity Analysis:
Model A: 5 variables, RÂ²/var = 0.1626
Model B: 7 variables, RÂ²/var = 0.1180
Model C: 5 variables, RÂ²/var = 0.1619

Nested Model Comparisons (F-tests):
Model A vs Model B:
  F-statistic: 51.3548
  p-value: 0.0000
  Model B significantly better
  Note: Model A vs C and Model B vs C are not nested comparisons
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
(b) Model Recommendation and Analysis
Statistical Criteria Analysis:

1. Goodness of Fit:
   â€¢ RÂ² ranking: Model B > others
   â€¢ Adjusted RÂ² ranking: Model B > others
   â€¢ RÂ² improvement from A to B: 0.0134
   â€¢ Adjusted RÂ² change from A to B: 0.0132

2. Model Parsimony:
   â€¢ AIC favors: Model B (AIC = 6472.65)
   â€¢ BIC favors: Model B (BIC = 6509.05)
   â€¢ BIC penalizes complexity more heavily than AIC

3. Coefficient Significance:
   â€¢ Model A: 5/5 coefficients significant (100.0\%)
   â€¢ Model B: 7/7 coefficients significant (100.0\%)
   â€¢ Model C: 5/5 coefficients significant (100.0\%)

4. Prediction Accuracy:
   â€¢ Lowest prediction error: Model B (SD = 2.6939)

Practical Interpretability Analysis:

1. Variable Inclusion Logic:
   â€¢ Model A: Core financial variables (deductible, coverage, premium) + risk
factors (age, prior\_claims)
   â€¢ Model B: Model A + property characteristics (type, location)
   â€¢ Model C: Simplified version with key variables + property type

2. Business Relevance:
   â€¢ Age variable: Present in A, Present in B, Absent in C
   â€¢ Property type: Absent in A, Present in B, Present in C
   â€¢ Location: Absent in A, Present in B, Absent in C

3. Marginal Contribution Analysis:
   â€¢ Adding type + location (B vs A): RÂ² improves by 0.0134
   â€¢ Adjusted RÂ² change: 0.0132 (improvement)

Recommendation Framework:

Composite Scoring (weighted combination of criteria):
   â€¢ Model B: 1.000
   â€¢ Model A: 0.700
   â€¢ Model C: 0.400

 RECOMMENDED MODEL: Model B

Justification for Model B:
   âœ“ Highest predictive power (RÂ² = 0.8263)
   âœ“ Includes important property characteristics
   âœ“ Comprehensive variable coverage
   âœ“ Best for prediction accuracy

Limitations of Model B:
    More complex with potential overfitting risk
    May have multicollinearity issues

Alternative Recommendations by Use Case:
   â€¢ For prediction accuracy: Model B
   â€¢ For model parsimony: Model B
   â€¢ For balanced approach: Model B
   â€¢ For regulatory reporting: Model A (simplest, most interpretable)
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \textbf{9 Practical Application}
    \begin{Verbatim}[commandchars=\\\{\}]
Features shape: (1340, 7)
Target shape: (1340,)

=== MODEL RESULTS ===
R-squared: 0.8263
Adjusted R-squared: 0.8254

Model Coefficients:
        Feature  Coefficient
0     Intercept     3.026950
1    deductible    -0.713391
2      coverage     0.058017
3           age     0.076546
4  prior\_claims     2.391648
5       premium     1.018707
6          type    -1.419290
7      location     0.858614

Statistical Significance:
        Feature  Coefficient  Std\_Error  t\_statistic       p\_value  \textbackslash{}
0     Intercept     3.026950   0.316198     9.572968  0.000000e+00
1    deductible    -0.713391   0.038024   -18.761697  0.000000e+00
2      coverage     0.058017   0.002180    26.618766  0.000000e+00
3           age     0.076546   0.006979    10.967841  0.000000e+00
4  prior\_claims     2.391648   0.124993    19.134246  0.000000e+00
5       premium     1.018707   0.237095     4.296623  1.860187e-05
6          type    -1.419290   0.169370    -8.379819  0.000000e+00
7      location     0.858614   0.172627     4.973820  7.420302e-07

   Significant
0         True
1         True
2         True
3         True
4         True
5         True
6         True
7         True

=== MODEL DIAGNOSTICS ===
Mean Squared Error: 7.2140
Root Mean Squared Error: 2.6859
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_63_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
=== CORRELATION ANALYSIS ===
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_63_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Correlations with Claims:
claims          1.000000
premium         0.792992
coverage        0.760527
prior\_claims    0.387403
deductible     -0.265120
age             0.198837
location        0.105441
type           -0.061114
Name: claims, dtype: float64

=== PART (a): PREDICTION ===
Understanding categorical variables:
Type values: [1 0]
Location values: [1 0]
Type value counts: type
1    835
0    505
Name: count, dtype: int64
Location value counts: location
1    968
0    372
Name: count, dtype: int64

Prediction for the given property:
Expected claims amount: 19.49

Sensitivity analysis for categorical variables:
  Type=Commercial, Location=Rural: 20.05
  Type=Commercial, Location=Urban: 20.91
  Type=Residential, Location=Rural: 18.63
  Type=Residential, Location=Urban: 19.49
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
PART (b): BUSINESS IMPLICATIONS AND RECOMMENDATIONS

Feature Importance (by absolute coefficient value):
        Feature  Coefficient  Abs\_Coefficient
3  prior\_claims     2.391648         2.391648
5          type    -1.419290         1.419290
4       premium     1.018707         1.018707
6      location     0.858614         0.858614
0    deductible    -0.713391         0.713391
2           age     0.076546         0.076546
1      coverage     0.058017         0.058017

Prediction Confidence Interval (95.0\%):
Expected claims: 19.49
Lower bound: 14.22
Upper bound: 24.76

----------------------------------------
BUSINESS RECOMMENDATIONS:
----------------------------------------

1. PRICING STRATEGY:
   - The model explains 82.6\% of the variation in claims
   - Most significant factors should drive premium calculations
   - Consider the prediction interval when setting reserves

2. RISK FACTORS ANALYSIS:
   Based on the coefficients, focus on:
   - Variables with largest absolute coefficients
   - Statistically significant predictors (p < 0.05)
   - High correlation factors with claims

3. UNDERWRITING GUIDELINES:
   - Properties with high predicted claims may need:
     * Higher premiums
     * Additional risk assessment
     * Different deductible structures
   - Consider segmented pricing models

4. PORTFOLIO MANAGEMENT:
   - Monitor actual vs predicted claims regularly
   - Update model coefficients as new data becomes available
   - Consider non-linear relationships or interaction terms

5. OPERATIONAL INSIGHTS:
   - Use model predictions for:
     * Reserve allocation
     * Risk-based pricing
     * Customer segmentation
     * Fraud detection (outliers in residuals)


OUTLIER ANALYSIS:
Properties with unusually high/low claims (>2 std devs): 66
These may require special investigation for:
- Fraud detection
- Model improvement opportunities
- Special risk factors not captured in current model
    \end{Verbatim}
    \textbf{10 Critical Thinking}
    \begin{Verbatim}[commandchars=\\\{\}]
PART (a) MULTIPLE LINEAR REGRESSION ASSUMPTIONS ANALYSIS

The key assumptions of multiple linear regression are:

1. LINEARITY: The relationship between predictors and response is linear
2. INDEPENDENCE: Observations are independent of each other
3. HOMOSCEDASTICITY: Constant variance of residuals (homogeneous variance)
4. NORMALITY: Residuals are normally distributed
5. NO MULTICOLLINEARITY: Predictors are not highly correlated with each other
6. NO OUTLIERS/INFLUENTIAL POINTS: Extreme values don't unduly influence the
model

Let's test each assumption:

1. LINEARITY ASSUMPTION
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
LINEARITY ASSESSMENT:
- Examine scatter plots for linear patterns
- Residuals vs Fitted should show random scatter around zero
- Non-linear patterns indicate violated linearity assumption

Correlations with claims:
  deductible: -0.265
  coverage: 0.761
  age: 0.199
  premium: 0.793

INSURANCE CONTEXT IMPLICATIONS:
- Insurance claims may have non-linear relationships (e.g., coverage thresholds)
- Age effects might be non-linear (newer vs very old properties)
- Premium-claims relationship might be non-linear due to risk-based pricing
2. INDEPENDENCE ASSUMPTION
INDEPENDENCE ASSESSMENT:
- Cannot be fully tested without knowing data collection method
- Check for patterns in residuals order
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Durbin-Watson statistic: 2.038
  (Values near 2.0 suggest independence, <1.5 or >2.5 suggest correlation)

INSURANCE CONTEXT IMPLICATIONS:
- Properties in same area might have correlated risks (floods, earthquakes)
- Temporal clustering if data spans multiple years with economic changes
- Policy renewals might create dependencies
3. HOMOSCEDASTICITY (CONSTANT VARIANCE) ASSUMPTION
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Breusch-Pagan test:
  LM statistic: 11.7914
  p-value: 0.1076
  Heteroscedasticity detected: No
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_7.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
INSURANCE CONTEXT IMPLICATIONS:
- Higher value properties might have more variable claims
- Heteroscedasticity common in insurance data
- May need weighted regression or transformation
4. NORMALITY OF RESIDUALS ASSUMPTION
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_9.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
NORMALITY TESTS:
Shapiro-Wilk test:
  Statistic: 0.8106, p-value: 0.0000
  Normal distribution: No

Jarque-Bera test:
  Statistic: 2188.1490, p-value: 0.0000
  Normal distribution: No

Descriptive statistics:
  Skewness: 1.9531
  Kurtosis: 4.8921

INSURANCE CONTEXT IMPLICATIONS:
- Insurance claims often right-skewed (many small, few large claims)
- May need log transformation or robust regression methods
- Non-normality affects confidence intervals and hypothesis tests
5. NO MULTICOLLINEARITY ASSUMPTION
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_11.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
HIGH CORRELATIONS (|r| > 0.7):
  coverage - premium: 0.723

VARIANCE INFLATION FACTORS:
       Variable        VIF
0    deductible   2.302515
1      coverage  36.103529
2           age   3.980363
3  prior\_claims   3.974976
4       premium  86.843712
5          type   3.287249
6      location   3.743009
  VIF > 5: Moderate multicollinearity
  VIF > 10: High multicollinearity

Variables with high VIF:
   Variable        VIF
1  coverage  36.103529
4   premium  86.843712

INSURANCE CONTEXT IMPLICATIONS:
- Premium and coverage likely correlated (higher coverage = higher premium)
- Deductible and coverage might be related
- Consider removing highly correlated variables or using regularization
6. NO OUTLIERS/INFLUENTIAL POINTS ASSUMPTION
OUTLIER DETECTION:
Observations with |standardized residuals| > 2.5: 48
Observations with high Cook's distance: 13
    \end{Verbatim}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_68_13.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
Outlier observations (standardized residuals > 2.5):
     claims  deductible  coverage  age  premium
0     22.67        1.44     165.7    2     2.23
182   18.12        5.67     131.1   30     2.05
203   35.65        5.08     378.5    8     4.39
247   18.22        7.30     214.5    1     2.29
269   28.41        1.20     209.1   13     2.89

INSURANCE CONTEXT IMPLICATIONS:
- Large claims are natural in insurance (catastrophic events)
- Outliers might represent legitimate extreme events, not errors
- Consider robust regression methods or separate models for extreme claims
OVERALL ASSUMPTION ASSESSMENT FOR INSURANCE CLAIMS

LIKELY VIOLATED ASSUMPTIONS:
1. Linearity: Insurance relationships often non-linear
2. Normality: Claims typically right-skewed
3. Homoscedasticity: Variance often increases with claim size
4. Independence: Geographic/temporal clustering possible

RECOMMENDED SOLUTIONS:
1. Log transformation of claims (handle skewness)
2. Robust regression methods
3. Polynomial or interaction terms
4. Weighted least squares (address heteroscedasticity)
5. Consider GLM (Gamma or Poisson regression)
6. Outlier-robust methods
    \end{Verbatim}
    \begin{Verbatim}[commandchars=\\\{\}]
PART (b) Additional Useful Variables
--------------------------------------------------

1. PROPERTY-SPECIFIC VARIABLES
â€¢ Construction: Building materials, roof type/age, year built, size, stories
â€¢ Condition: Recent renovations, security features, maintenance score

2. ENVIRONMENTAL \& GEOGRAPHIC
â€¢ Climate: Climate zones, precipitation, natural disaster scores
â€¢ Location: Crime rates, distance to fire station/water, building codes

3. ECONOMIC \& DEMOGRAPHIC
â€¢ Economic: Local income, property appreciation, unemployment rate
â€¢ Demographics: Owner vs tenant occupied, primary vs secondary residence

4. USAGE \& BEHAVIORAL
â€¢ Property Use: Home business, rental income, vacancy duration
â€¢ Claims History: Previous claim types, time since last claim
â€¢ Behavior: Payment history, policy shopping, service interactions

5. ADVANCED MODELING
â€¢ Interaction Effects: AgeÃ—Construction, LocationÃ—Weather, CoverageÃ—Deductible
â€¢ External Data: Credit scores, satellite imagery, weather APIs

6. IMPLEMENTATION PRIORITY
â€¢ HIGH: Natural disaster scores, construction details, claims history
â€¢ MEDIUM: Neighborhood data, weather variables, usage patterns
â€¢ LOW: Credit indicators, satellite analysis, economic metrics

7. EXPECTED OUTCOMES
â€¢ Model Accuracy: 60-80\% â†’ 85-95\% predictive accuracy
â€¢ Benefits: Better risk selection, fraud detection, dynamic pricing

8. KEY CONSIDERATIONS
â€¢ Data availability varies by property
â€¢ Quality validation required for third-party data
â€¢ Regulatory compliance (fair housing laws)
â€¢ Cost-benefit analysis essential
    \end{Verbatim}
\end{document}
