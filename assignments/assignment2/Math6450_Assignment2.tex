\documentclass[8pt, twocolumn]{extarticle}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    % UNICODE CHARACTER SUPPORT - FIX FOR MISSING CHARACTERS
    % Add this section to handle Unicode characters in Verbatim environments
    \usepackage{newunicodechar}
    
    % Define replacements for problematic Unicode characters
    \newunicodechar{ρ}{\ensuremath{\rho}}
    \newunicodechar{≈}{\ensuremath{\approx}}
    \newunicodechar{≤}{\ensuremath{\leq}}
    \newunicodechar{≥}{\ensuremath{\geq}}
    \newunicodechar{×}{\ensuremath{\times}}
    \newunicodechar{β}{\ensuremath{\beta}}
    \newunicodechar{α}{\ensuremath{\alpha}}
    \newunicodechar{²}{\ensuremath{^2}}
    \newunicodechar{₀}{\ensuremath{_0}}
    \newunicodechar{₁}{\ensuremath{_1}}
    \newunicodechar{₂}{\ensuremath{_2}}
    \newunicodechar{≠}{\ensuremath{\neq}}     % Not equal
    \newunicodechar{✓}{\checkmark}            % Check mark
    % END UNICODE CHARACTER SUPPORT
    
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \makeatletter
    \newsavebox\pandoc@box
    \newcommand*\pandocbounded[1]{%
      \sbox\pandoc@box{#1}%
      % scaling factors for width and height
      \Gscale@div\@tempa\textheight{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
      \Gscale@div\@tempb\linewidth{\wd\pandoc@box}%
      % select the smaller of both
      \ifdim\@tempb\p@<\@tempa\p@
        \let\@tempa\@tempb
      \fi
      % scaling accordingly (\@tempa < 1)
      \ifdim\@tempa\p@<\p@
        \scalebox{\@tempa}{\usebox\pandoc@box}%
      % scaling not needed, use as it is
      \else
        \usebox{\pandoc@box}%
      \fi
    }
    \makeatother

    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Math6450\_Assignment2}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    1 Data Exploration

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Calculate and report the descriptive statistics (mean, median,
  standard deviation, minimum, maximum) for all continuous variables in
  the dataset.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
PropertyFund Dataset Analysis
==================================================

(a) Descriptive Statistics for Continuous Variables
--------------------------------------------------

Comprehensive Descriptive Statistics:
               Mean   Median  Std Dev  Minimum  Maximum  Skewness  Kurtosis
claims       18.049   17.845    6.448     0.72    41.39     0.254     0.095
deductible    2.490    1.905    1.942     0.51    10.00     1.542     2.351
coverage    189.014  186.750   72.169    50.00   424.50     0.145    -0.292
age          15.438   11.000   14.227     1.00    85.00     1.869     4.496
premium       2.969    2.945    0.822     0.50     5.78     0.245     0.030
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Create a correlation matrix for all continuous variables. Which
  variable has the strongest linear relationship with claims?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(b) Correlation Matrix for Continuous Variables
--------------------------------------------------

Correlation Matrix:
            claims  deductible  coverage    age  premium
claims       1.000      -0.265     0.761  0.199    0.793
deductible  -0.265       1.000    -0.066  0.006   -0.059
coverage     0.761      -0.066     1.000 -0.015    0.723
age          0.199       0.006    -0.015  1.000    0.314
premium      0.793      -0.059     0.723  0.314    1.000

Variable with strongest linear relationship with 'claims':
Variable: premium
Correlation coefficient: 0.793
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Identify any variables that appear to have skewed distributions based
  on the descriptive statistics. For these variables, comment on whether
  a logarithmic transformation might be appropriate.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(c) Skewness Analysis and Log Transformation Assessment
--------------------------------------------------

Skewness Assessment:
Rule of thumb: |skewness| > 1 indicates highly skewed distribution
Rule of thumb: 0.5 < |skewness| < 1 indicates moderately skewed distribution

claims:
  Skewness: 0.254
  Assessment: Approximately symmetric

deductible:
  Skewness: 1.542
  Assessment: Highly skewed
  Log transformation skewness: 0.134
  Improvement from log transformation: 1.408
  Recommendation: Log transformation would improve normality

coverage:
  Skewness: 0.145
  Assessment: Approximately symmetric

age:
  Skewness: 1.869
  Assessment: Highly skewed
  Log transformation skewness: -0.347
  Improvement from log transformation: 1.523
  Recommendation: Log transformation would improve normality

premium:
  Skewness: 0.245
  Assessment: Approximately symmetric

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_7_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]

Summary of Findings:
------------------------------
Variables with skewed distributions: deductible, age
Variable most strongly correlated with claims: premium (r = 0.793)

Data Overview:
Total observations: 1,340
Variables analyzed: 5
Missing values: 0
    \end{Verbatim}

    2 Simple Regression Analysis

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Fit a simple linear regression model with claims as the dependent
  variable and coverage as the explanatory variable. Write the fitted
  regression equation.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Simple Linear Regression Analysis: Claims vs Coverage
============================================================
Dataset Information:
Total observations: 1,340
Observations used in regression: 1,340
Missing values removed: 0

(a) Simple Linear Regression Model Fitting
--------------------------------------------------
Model Coefficients:
Intercept (β₀): 5.2054
Slope (β₁): 0.0679

Fitted Regression Equation:
Claims = 5.2054 + 0.0679 × Coverage

In mathematical notation:
ŷ = 5.2054 + 0.0679x
where ŷ = predicted claims, x = coverage
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Interpret the slope coefficient in practical terms. What does it tell
  us about the relationship between coverage and claims?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(b) Interpretation of Slope Coefficient
--------------------------------------------------
Slope coefficient: 0.0679

Practical Interpretation:
• For every 1-unit increase in coverage, claims are expected to increase by
0.0679 units, on average.
• This indicates a positive relationship between coverage and claims.
• Properties with higher coverage amounts tend to have higher claims.

Alternative interpretation:
• For every 100-unit increase in coverage, claims change by 6.79 units, on
average.

Example predictions:
• Coverage = 100: Predicted Claims = 12.00
• Coverage = 150: Predicted Claims = 15.40
• Coverage = 200: Predicted Claims = 18.80
• Coverage = 250: Predicted Claims = 22.19
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Calculate and interpret the coefficient of determination (R2) for this
  model.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(c) Coefficient of Determination (R²) Analysis
--------------------------------------------------
Model Performance Metrics:
R² (Coefficient of Determination): 0.5784
R² as percentage: 57.84\%
Correlation coefficient (r): 0.7605
Root Mean Square Error (RMSE): 4.1850

Interpretation of R²:
• 57.84\% of the variation in claims is explained by coverage.
• 42.16\% of the variation in claims is due to other factors not included in the
model.
• The linear relationship between coverage and claims is moderate (R² = 0.5784).

Statistical Significance:
• t-statistic: 42.8442
• p-value: 0.0000
• Degrees of freedom: 1338
• The relationship is statistically significant at the 5\% level.
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]


Summary Table:
         Metric  Value                                 Interpretation
 Intercept (β₀) 5.2054              Expected claims when coverage = 0
     Slope (β₁) 0.0679 Change in claims per unit increase in coverage
             R² 0.5784                    57.8\% of variance explained
Correlation (r) 0.7605                    Linear association strength
           RMSE 4.1850                       Average prediction error
   Observations   1340                                    Sample size
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]


Key Findings Summary:
• Regression equation: Claims = 5.2054 + 0.0679 × Coverage
• Slope interpretation: Each additional unit of coverage is associated with a
0.0679 unit change in claims
• Model explains 57.8\% of the variation in claims
• The relationship is statistically significant (p = 0.0000)
    \end{Verbatim}

    3 Multiple Regression Model

Fit a multiple linear regression model with claims as the dependent
variable and the following explanatory variables: deductible, coverage,
age, prior claims, and premium.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Write the fitted regression equation with coefficient estimates
  rounded to 3 decimal places.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Multiple Linear Regression Analysis
==================================================
Dependent Variable: claims
Explanatory Variables: deductible, coverage, age, prior\_claims, premium

Dataset Information:
Total observations: 1,340
Complete cases used: 1,340
Observations removed (missing data): 0
Number of explanatory variables: 5

(a) Fitted Regression Equation
----------------------------------------
Coefficient Estimates (rounded to 3 decimal places):
Intercept (β₀): 3.208
β\_1 (deductible): -0.728
β\_2 (coverage): 0.062
β\_3 (age): 0.091
β\_4 (prior\_claims): 2.580
β\_5 (premium): 0.495

Fitted Regression Equation:
Claims = 3.208 - 0.728 × deductible + 0.062 × coverage + 0.091 × age + 2.580 ×
prior\_claims + 0.495 × premium

Compact Mathematical Form:
ŷ = β₀ + β₁x₁ + β₂x₂ + β₃x₃ + β₄x₄ + β₅x₅
ŷ = 3.208 + -0.728x₁ + 0.062x₂ + 0.091x₃ + 2.580x₄ + 0.495x₅
where x₁=deductible, x₂=coverage, x₃=age, x₄=prior\_claims, x₅=premium
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Report the standard errors for each coefficient.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(b) Standard Errors for Each Coefficient
----------------------------------------
Standard Errors:
Intercept (β₀): 0.3172
β\_1 (deductible): 0.0394
β\_2 (coverage): 0.0020
β\_3 (age): 0.0068
β\_4 (prior\_claims): 0.1210
β\_5 (premium): 0.2118

Additional Statistics (t-statistics and p-values):
Coefficient     Estimate   Std Error    t-stat     p-value    Significance
---------------------------------------------------------------------------
Intercept       3.208      0.3172       10.113     0.0000     ***
deductible      -0.728     0.0394       -18.459    0.0000     ***
coverage        0.062      0.0020       30.624     0.0000     ***
age             0.091      0.0068       13.401     0.0000     ***
prior\_claims    2.580      0.1210       21.316     0.0000     ***
premium         0.495      0.2118       2.338      0.0195     *
Significance codes: *** p<0.001, ** p<0.01, * p<0.05
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Calculate and report R2, adjusted R2, and the residual standard
  deviation.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(c) Model Performance Statistics
----------------------------------------
R² (Coefficient of Determination): 0.8130
Adjusted R²: 0.8123
Residual Standard Deviation: 2.7938

Additional Model Statistics:
Multiple R (Correlation): 0.9016
Residual Sum of Squares (RSS): 10412.1409
Mean Squared Error (MSE): 7.8052
F-statistic: 1159.6202
F-statistic p-value: 0.000000
Overall model significance: Yes (α = 0.05)

Degrees of Freedom:
Model: 5
Residual: 1334
Total: 1339

Summary Results Table:
       Variable  Coefficient  Std\_Error  Coefficient\_Rounded
0     Intercept       3.2078     0.3172                3.208
1    deductible      -0.7278     0.0394               -0.728
2      coverage       0.0621     0.0020                0.062
3           age       0.0906     0.0068                0.091
4  prior\_claims       2.5797     0.1210                2.580
5       premium       0.4953     0.2118                0.495

Model Performance Table:
             Statistic     Value
                    R²    0.8130
           Adjusted R²    0.8123
Residual Std Deviation    2.7938
           F-statistic 1159.6202
      p-value (F-test)  0.000000
          Observations      1340
             Variables         5
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]

Key Results Summary:
==================================================
✓ Multiple regression equation fitted with 5 explanatory variables
✓ Model explains 81.3\% of variance in claims (R² = 0.8130)
✓ Adjusted R² = 0.8123 (accounts for number of variables)
✓ Residual standard deviation = 2.7938
✓ Overall model is significant (F-test p-value = 0.000000)
✓ Standard errors calculated for all 6 coefficients
    \end{Verbatim}

    4 Statistical Inference

Using the multiple regression model from Question 3:

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Test whether the coefficient for age is statistically significant at
  the 5\% level. State your null and alternative hypotheses, calculate
  the t-statistic, and state your conclusion.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Statistical Inference and Hypothesis Testing
Multiple Linear Regression Model: Claims vs (Deductible, Coverage, Age,
Prior\_Claims, Premium)
================================================================================
==========
Model Summary:
Observations: 1340
Variables: 5
Degrees of freedom (residual): 1334
R²: 0.8130
MSE: 7.8052

Coefficient Estimates:
Variable        Coefficient  Std Error    t-statistic  p-value
---------------------------------------------------------------------------
deductible      -0.7278      0.0394       -18.4591     0.0000
coverage        0.0621       0.0020       30.6239      0.0000
age             0.0906       0.0068       13.4010      0.0000
prior\_claims    2.5797       0.1210       21.3156      0.0000
premium         0.4953       0.2118       2.3382       0.0195

(a) Testing Significance of Age Coefficient
==================================================
Hypothesis Test for Age Coefficient:

Null Hypothesis (H₀): β\_age = 0
Alternative Hypothesis (H₁): β\_age ≠ 0
Significance level (α): 0.05
Test type: Two-tailed t-test

Test Statistics:
Age coefficient (β\_age): 0.0906
Standard error (SE): 0.0068
t-statistic: 13.4010
Degrees of freedom: 1334
p-value: 0.0000
Critical value (±): 1.9617

Decision Rule:
Reject H₀ if |t-statistic| > 1.9617 OR if p-value < 0.05

Conclusion:
✓ REJECT H₀: The coefficient for age IS statistically significant at the 5\%
level.
  |t-statistic| = 13.4010 > 1.9617
  p-value = 0.0000 < 0.05
  Age has a statistically significant effect on claims.
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Construct a 95\% confidence interval for the coefficient of prior
  claims. Interpret this interval in practical terms.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(b) 95\% Confidence Interval for Prior Claims Coefficient
============================================================
Confidence Interval Calculation:
Coefficient (β\_prior\_claims): 2.5797
Standard error: 0.1210
Degrees of freedom: 1334
Confidence level: 95\%

Confidence Interval Formula:
CI = β̂ ± t\_(α/2,df) × SE(β̂)
CI = 2.5797 ± 1.9617 × 0.1210
CI = 2.5797 ± 0.2374

95\% Confidence Interval for Prior Claims Coefficient:
[2.3423, 2.8171]

Practical Interpretation:
• We are 95\% confident that the true effect of having prior claims on current
claims
  is between 2.3423 and 2.8171 units.
• Since the entire interval is positive, prior claims consistently INCREASE
current claims.
• Properties with prior claims have significantly higher current claims than
those without.
• The width of the interval (0.4748) indicates the precision of our estimate.
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Perform an overall F-test for the significance of the regression
  model. State your hypotheses, report the F-statistic and p-value, and
  draw your conclusion.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(c) Overall F-test for Model Significance
==================================================
Overall F-test for Regression Model:

Null Hypothesis (H₀): β₁ = β₂ = β₃ = β₄ = β₅ = 0
  (All explanatory variables have no effect on claims)
Alternative Hypothesis (H₁): At least one βᵢ ≠ 0
  (At least one explanatory variable has a significant effect)
Significance level (α): 0.05

Test Statistics:
Total Sum of Squares (TSS): 55667.4953
Explained Sum of Squares (ESS): 45255.3543
Residual Sum of Squares (RSS): 10412.1409
Mean Square Regression (MSR): 9051.0709
Mean Square Error (MSE): 7.8052

F-statistic: 1159.6202
Degrees of freedom: (5, 1334)
p-value: 0.000000
Critical F-value (α = 0.05): 2.2208

Decision Rule:
Reject H₀ if F-statistic > 2.2208 OR if p-value < 0.05

Conclusion:
✓ REJECT H₀: The regression model IS statistically significant at the 5\% level.
  F-statistic = 1159.6202 > 2.2208
  p-value = 0.000000 < 0.05
  At least one explanatory variable has a significant effect on claims.
  The model explains a significant portion of the variation in claims.

Model Performance Context:
R² = 0.8130 (81.3\% of variance explained)
The model performs well in predicting claims.
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]


Summary of All Statistical Tests:
============================================================
                    Test             Statistic  p-value         Conclusion
Age Coefficient (t-test)           t = 13.4010   0.0000        Significant
         Prior Claims CI CI = [2.3423, 2.8171]      N/A Does not contain 0
  Overall Model (F-test)         F = 1159.6202 0.000000  Model Significant

LaTeX Summary Table:
\textbackslash{}begin\{table\}
\textbackslash{}caption\{Summary of Statistical Tests\}
\textbackslash{}label\{tab:hypothesis\_tests\}
\textbackslash{}begin\{tabular\}\{llll\}
\textbackslash{}toprule
Test \& Statistic \& p-value \& Conclusion \textbackslash{}\textbackslash{}
\textbackslash{}midrule
Age Coefficient (t-test) \& t = 13.4010 \& 0.0000 \& Significant \textbackslash{}\textbackslash{}
Prior Claims CI \& CI = [2.3423, 2.8171] \& N/A \& Does not contain 0 \textbackslash{}\textbackslash{}
Overall Model (F-test) \& F = 1159.6202 \& 0.000000 \& Model Significant \textbackslash{}\textbackslash{}
\textbackslash{}bottomrule
\textbackslash{}end\{tabular\}
\textbackslash{}end\{table\}

    \end{Verbatim}

    5 Binary Variables and Model Interpretation

Add the binary variables type and location to your model from Question
3.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Write the new fitted regression equation.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Extended Multiple Linear Regression Analysis with Binary Variables
======================================================================
Adding 'type' and 'location' to the original model
Dependent Variable: claims
Original Variables: deductible, coverage, age, prior\_claims, premium
New Variables: type, location

Data Summary:
Original model observations: 1,340
Extended model observations: 1,340

Extended Model Summary:
Observations: 1340
Variables: 7
R²: 0.8263
Adjusted R²: 0.8254
Residual Standard Error: 2.6939

(a) Extended Regression Model Equation
==================================================
Coefficient Estimates:
Variable        Coefficient  Std Error    t-stat     p-value
----------------------------------------------------------------------
Intercept       3.027        0.3171
deductible      -0.713       0.0381       -18.706    0.0000
coverage        0.058        0.0022       26.539     0.0000
age             0.077        0.0070       10.935     0.0000
prior\_claims    2.392        0.1254       19.077     0.0000
premium         1.019        0.2378       4.284      0.0000
type            -1.419       0.1699       -8.355     0.0000
location        0.859        0.1731       4.959      0.0000

Fitted Regression Equation:
Claims = 3.027 - 0.713 × deductible + 0.058 × coverage + 0.077 × age + 2.392 ×
prior\_claims + 1.019 × premium - 1.419 × type + 0.859 × location

Detailed Mathematical Form:
Claims = 3.027 + -0.713×deductible + 0.058×coverage
         + 0.077×age + 2.392×prior\_claims + 1.019×premium
         + -1.419×type + 0.859×location
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Interpret the coefficient for type in practical terms. How much higher
  or lower are claims for residential properties compared to commercial
  properties, holding all other variables constant?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(b) Interpretation of Type Coefficient
=============================================
Type Coefficient Analysis:
Coefficient (β\_type): -1.419
Standard Error: 0.1699
t-statistic: -8.355
p-value: 0.0000

Type variable coding: [0, 1]

Practical Interpretation:
• Properties with type = 1 have claims that are 1.419 units LOWER than
properties with type = 0,
  holding all other variables constant.

Assuming standard coding (0 = Commercial, 1 = Residential):
• Residential properties have claims that are 1.419 units lower than commercial
properties.
• This suggests commercial properties are associated with higher insurance
claims.

Statistical Significance:
• The type coefficient IS statistically significant (p = 0.0000 < 0.05)
• We can be confident that property type has a real effect on claims.
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Test whether the addition of type and location significantly improves
  the model using a partial F-test. Compare the R2 values and comment on
  the improvement.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(c) Partial F-test for Model Improvement
=============================================
Model Comparison (same sample size: 1340):
Model                R²           Adj R²       Variables  RSS
----------------------------------------------------------------------
Original             0.8130       0.8123       5          10412.1409
Extended             0.8263       0.8254       7          9666.7444

R² Improvement: 0.0134 (1.34 percentage points)

Partial F-test:
H₀: β\_type = β\_location = 0 (binary variables add no explanatory power)
H₁: At least one of β\_type or β\_location ≠ 0 (binary variables improve the
model)

Partial F-test Calculations:
RSS(original): 10412.1409
RSS(extended): 9666.7444
Reduction in RSS: 745.3965
Additional variables (q): 2
DF residual (extended): 1332

F-statistic: 51.3548
Degrees of freedom: (2, 1332)
p-value: 0.0000
Critical F-value (α = 0.05): 3.0025

Conclusion:
✓ REJECT H₀: Adding type and location SIGNIFICANTLY improves the model
  F = 51.3548 > 3.0025
  p-value = 0.0000 < 0.05
  The binary variables provide significant additional explanatory power.

Model Improvement Assessment:
• R² improved by 0.0134 (1.34 percentage points) - this is modest
• Extended model explains 82.6\% vs 81.3\% of variance
• Adjusted R² increased from 0.8123 to 0.8254
• The improvement in adjusted R² suggests the added variables are worthwhile
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_39_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]


Executive Summary:
==================================================
                  Aspect                                             Finding
 Extended Model Equation Claims = 3.027 + {\ldots} + -1.419×type + 0.859×location
        Type Coefficient                                              -1.419
             Type Effect                       Type=1 has 1.419 lower claims
Statistical Significance                            Significant (p = 0.0000)
          R² Improvement                     0.0134 (1.34 percentage points)
   Partial F-test Result                Significant improvement (p = 0.0000)
    \end{Verbatim}

    6 Interaction Effects

Create a new model that includes an interaction term between deductible
and type.

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Write the regression function that includes this interaction term.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Regression Model with Interaction Term: Deductible × Type
=================================================================
Model Features: deductible, type, coverage, age, prior\_claims, premium
Interaction Term: deductible × type

Data Summary:
Total observations: 1,340
Complete cases used: 1,340
Missing values removed: 0
Type variable coding: [0, 1]

Interaction Term (deductible × type) Statistics:
Mean: 1.5335
Std Dev: 1.9042
Range: [0.0000, 10.0000]

Model Summary:
R²: 0.8233
Adjusted R²: 0.8224
Residual Standard Error: 2.7172
F-statistic: 886.8341

Coefficient Estimates:
Variable           Coefficient  Std Error    t-stat     p-value    Sig
--------------------------------------------------------------------------------
Intercept          3.2856       0.3300
deductible         -0.6729      0.0596       -11.2894   0.0000     ***
type               -1.2573      0.2598       -4.8392    0.0000     ***
coverage           0.0553       0.0021       25.9580    0.0000     ***
age                0.0703       0.0070       10.1034    0.0000     ***
prior\_claims       2.2568       0.1234       18.2905    0.0000     ***
premium            1.3647       0.2290       5.9595     0.0000     ***
deductible\_x\_type  -0.0946      0.0779       -1.2151    0.2245
Significance codes: *** p<0.001, ** p<0.01, * p<0.05

(a) Regression Function with Interaction Term
==================================================
General Form:
Claims = β₀ + β₁×deductible + β₂×type + β₃×coverage + β₄×age + β₅×prior\_claims +
β₆×premium + β₇×(deductible×type) + ε

Fitted Regression Equation:
Claims = 3.2856 - 0.6729×deductible - 1.2573×type + 0.0553×coverage + 0.0703×age
+ 2.2568×prior\_claims + 1.3647×premium - 0.0946×(deductible×type)

With Coefficient Values:
Claims = 3.2856 + -0.6729×deductible + -1.2573×type
         + 0.0553×coverage + 0.0703×age + 2.2568×prior\_claims
         + 1.3647×premium + -0.0946×(deductible×type)
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Interpret how the effect of deductible on claims differs between
  residential and commercial properties.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(b) Interpretation of Deductible Effect by Property Type
============================================================
Key Coefficients:
β₁ (deductible): -0.6729
β₂ (type): -1.2573
β₇ (deductible×type): -0.0946

Interpretation of Interaction Effect:
The interaction model allows the effect of deductible to differ by property
type.

For Commercial Properties (type = 0):
∂Claims/∂deductible = β₁ + β₇×0 = β₁ = -0.6729
• A 1-unit increase in deductible changes claims by -0.6729 units for commercial
properties.

For Residential Properties (type = 1):
∂Claims/∂deductible = β₁ + β₇×1 = β₁ + β₇ = -0.6729 + -0.0946 = -0.7675
• A 1-unit increase in deductible changes claims by -0.7675 units for
residential properties.

Comparison:
Difference in deductible effect: -0.0946
• The deductible effect is 0.0946 units MORE NEGATIVE for residential
properties.
• Deductible increases have a stronger negative effect on residential claims
than commercial claims.

Practical Business Interpretation:
• Higher deductibles are associated with lower claims for both property types
• This association is STRONGER for residential properties
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Test whether the interaction term is statistically significant at the
  5\% level.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(c) Statistical Significance Test for Interaction Term
============================================================
Hypothesis Test for Interaction Term:
H₀: β₇ = 0 (no interaction between deductible and type)
H₁: β₇ ≠ 0 (significant interaction exists)
Significance level: α = 0.05

Test Statistics:
Interaction coefficient (β₇): -0.0946
Standard error: 0.0779
t-statistic: -1.2151
Degrees of freedom: 1332
p-value: 0.2245
Critical value (±): 1.9617

Decision Rule:
Reject H₀ if |t-statistic| > 1.9617 OR if p-value < 0.05

Conclusion:
✗ FAIL TO REJECT H₀: The interaction term is NOT statistically significant at
the 5\% level.
  |t-statistic| = 1.2151 ≤ 1.9617
  p-value = 0.2245 ≥ 0.05
  The effect of deductible on claims does NOT differ significantly between
property types.
  The interaction term may not be necessary.

95\% Confidence Interval for Interaction Coefficient:
[-0.2473, 0.0581]
• The interval contains zero - the direction of the interaction effect is
uncertain
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_47_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]


Executive Summary:
==================================================
                  Aspect
Result
     Model Specification Claims \textasciitilde{} deductible + type + coverage + age +
prior\_claims + premium + deductible×type
 Interaction Coefficient
-0.0946 (SE = 0.0779)
       Commercial Effect
-0.6729 per unit deductible
      Residential Effect
-0.7675 per unit deductible
              Difference
-0.0946
Statistical Significance
Not significant (p = 0.2245)
                Model R²
0.8233

Model Interpretation:
• The non-significant interaction suggests that deductible effects are
  similar across commercial and residential properties
• A simpler model without interaction may be adequate
    \end{Verbatim}

    7 Residual Analysis

Using your model from Question 5:

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Create a plot of residuals versus fitted values. Comment on any
  patterns you observe.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Residual Analysis and Model Diagnostics
Extended Multiple Linear Regression Model
============================================================
Variables: deductible, coverage, age, prior\_claims, premium, type, location
Model Summary:
Observations: 1,340
Variables: 7
R²: 0.8263
Residual Standard Error: 2.6939

(a) Residuals vs Fitted Values Analysis
=============================================
Residuals vs Fitted Values Analysis:
Residual range: [-3.376, 15.203]
Fitted values range: [0.792, 39.985]

Pattern Analysis:
Correlation between fitted values and squared residuals: 0.0310
• Variance appears roughly constant
• Correlation magnitude suggests homoscedasticity (constant variance)

Linearity Assessment:
Mean residuals by fitted value terciles:
• Low tercile: -0.0800
• Middle tercile: 0.0229
• High tercile: 0.0572
• Maximum deviation from zero: 0.0800 (suggests linear relationship is
appropriate)
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Create a Q-Q plot of the residuals. Does the normality assumption
  appear to be satisfied?
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(b) Q-Q Plot and Normality Analysis
========================================
Normality Test Results:
Shapiro-Wilk Test:
  Statistic: 0.8106
  p-value: 0.0000
  REJECT normality at α=0.05

Jarque-Bera Test:
  Statistic: 2188.1490
  p-value: 0.0000
  REJECT normality at α=0.05

Kolmogorov-Smirnov Test:
  Statistic: 0.1468
  p-value: 0.0000
  REJECT normality at α=0.05

Descriptive Statistics for Normality:
Skewness: 1.9531 (Normal ≈ 0)
Kurtosis: 4.8921 (Normal ≈ 0)
Skewness interpretation: highly skewed
Kurtosis interpretation: heavy-tailed

Overall Normality Assessment: Assumption appears to be violated
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Identify any observations that might be outliers or influential points
  based on your residual analysis.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]

(c) Outliers and Influential Points Analysis
==================================================
Diagnostic Thresholds:
Outlier threshold (standardized residuals): ±3
High leverage threshold: 0.0119
High Cook's distance threshold: 0.0030

Outliers and Influential Points:
Observations with |standardized residuals| > 3: 31
Observations with |studentized residuals| > 3: 31
High leverage points: 73
High Cook's distance points: 74

Most Extreme Observations:
Highest Residual: Observation 315
  Fitted value: 23.547
  Actual value: 38.750
  Standardized residual: 5.643
  Leverage: 0.0072
  Cook's distance: 0.0331
Highest Leverage: Observation 262
  Fitted value: 34.070
  Actual value: 36.160
  Standardized residual: 0.776
  Leverage: 0.0305
  Cook's distance: 0.0027
Highest Cooks: Observation 315
  Fitted value: 23.547
  Actual value: 38.750
  Standardized residual: 5.643
  Leverage: 0.0072
  Cook's distance: 0.0331
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
<Figure size 640x480 with 0 Axes>
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

Detailed Analysis of Problematic Observations:
------------------------------------------------------------
 Obs Fitted Actual Std\_Residual Leverage Cooks\_D                 Issues
   1 13.477 22.670        3.412   0.0032  0.0054 Outlier, High Cook's D
   2  5.711  3.340       -0.880   0.0122  0.0014          High Leverage
  14 20.959 20.000       -0.356   0.0128  0.0002          High Leverage
  36 10.929  8.700       -0.827   0.0142  0.0014          High Leverage
  70 13.967 11.670       -0.852   0.0130  0.0014          High Leverage
  71 20.337 24.990        1.727   0.0074  0.0032          High Cook's D
  73 30.965 29.670       -0.481   0.0141  0.0005          High Leverage
 118 22.728 22.290       -0.163   0.0193  0.0001          High Leverage
 122  5.247 10.110        1.805   0.0072  0.0034          High Cook's D
 129 31.861 36.730        1.807   0.0092  0.0043          High Cook's D

{\ldots} and 124 more observations with issues.


Diagnostic Summary:
========================================
1. Linearity: suggests linear relationship is appropriate
2. Homoscedasticity: suggests homoscedasticity (constant variance)
3. Normality: Assumption appears to be violated
4. Outliers: 31 potential outliers identified
5. Influential Points: 74 high Cook's distance observations

Recommendations:
• Consider transformation of variables or robust regression methods
• Examine influential points - consider their impact on coefficient estimates
    \end{Verbatim}

    8 Model Comparison and Selection

Compare three models

Model A: claims ∼ deductible + coverage + age + prior claims + premium

Model B: claims ∼ deductible + coverage + age + prior claims + premium +
type + location

Model C: claims ∼ deductible + coverage + prior claims + premium + type

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Create a table comparing the R2, adjusted R2, and residual standard
  deviation for all three models.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
Model Comparison and Selection Analysis
============================================================
Comparing three different model specifications:
Model A: claims \textasciitilde{} deductible + coverage + age + prior\_claims + premium
Model B: claims \textasciitilde{} deductible + coverage + age + prior\_claims + premium + type +
location
Model C: claims \textasciitilde{} deductible + coverage + prior\_claims + premium + type

Data Summary:
Original dataset size: 1,340
Complete cases for all models: 1,340
Cases removed due to missing data: 0

-------------------- Model A --------------------
Variables: deductible, coverage, age, prior\_claims, premium
Number of variables: 5
R²: 0.8130
Adjusted R²: 0.8123
Residual Standard Deviation: 2.7938
AIC: 6566.17
BIC: 6592.18
Significant coefficients (p < 0.05): 5/5

-------------------- Model B --------------------
Variables: deductible, coverage, age, prior\_claims, premium, type, location
Number of variables: 7
R²: 0.8263
Adjusted R²: 0.8254
Residual Standard Deviation: 2.6939
AIC: 6472.65
BIC: 6509.05
Significant coefficients (p < 0.05): 7/7

-------------------- Model C --------------------
Variables: deductible, coverage, prior\_claims, premium, type
Number of variables: 5
R²: 0.8095
Adjusted R²: 0.8088
Residual Standard Deviation: 2.8197
AIC: 6590.93
BIC: 6616.94
Significant coefficients (p < 0.05): 5/5


(a) Model Comparison Table
==================================================
Primary Comparison Metrics:
  Model Variables     R²  Adj\_R²  Residual\_SD
Model A    5 vars 0.8130  0.8123       2.7938
Model B    7 vars 0.8263  0.8254       2.6939
Model C    5 vars 0.8095  0.8088       2.8197

Additional Model Selection Criteria:
  Model     AIC     BIC  F\_statistic Sig\_Coefs
Model A 6566.17 6592.18      1159.62       5/5
Model B 6472.65 6509.05       905.51       7/7
Model C 6590.93 6616.94      1133.51       5/5

Best Model by Criterion:
• Highest R²: Model B (0.8263)
• Highest Adjusted R²: Model B (0.8254)
• Lowest Residual SD: Model B (2.6939)
• Lowest AIC: Model B (6472.65)
• Lowest BIC: Model B (6509.05)

Model Complexity Analysis:
Model A: 5 variables, R²/var = 0.1626
Model B: 7 variables, R²/var = 0.1180
Model C: 5 variables, R²/var = 0.1619

Nested Model Comparisons (F-tests):
Model A vs Model B:
  F-statistic: 51.3548
  p-value: 0.0000
  Model B significantly better
  Note: Model A vs C and Model B vs C are not nested comparisons
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Which model would you recommend and why? Consider both statistical
  criteria and practical interpretability.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]


(b) Model Recommendation and Analysis
==================================================
Statistical Criteria Analysis:

1. Goodness of Fit:
   • R² ranking: Model B > others
   • Adjusted R² ranking: Model B > others
   • R² improvement from A to B: 0.0134
   • Adjusted R² change from A to B: 0.0132

2. Model Parsimony:
   • AIC favors: Model B (AIC = 6472.65)
   • BIC favors: Model B (BIC = 6509.05)
   • BIC penalizes complexity more heavily than AIC

3. Coefficient Significance:
   • Model A: 5/5 coefficients significant (100.0\%)
   • Model B: 7/7 coefficients significant (100.0\%)
   • Model C: 5/5 coefficients significant (100.0\%)

4. Prediction Accuracy:
   • Lowest prediction error: Model B (SD = 2.6939)

Practical Interpretability Analysis:

1. Variable Inclusion Logic:
   • Model A: Core financial variables (deductible, coverage, premium) + risk
factors (age, prior\_claims)
   • Model B: Model A + property characteristics (type, location)
   • Model C: Simplified version with key variables + property type

2. Business Relevance:
   • Age variable: Present in A, Present in B, Absent in C
   • Property type: Absent in A, Present in B, Present in C
   • Location: Absent in A, Present in B, Absent in C

3. Marginal Contribution Analysis:
   • Adding type + location (B vs A): R² improves by 0.0134
   • Adjusted R² change: 0.0132 (improvement)

Recommendation Framework:

Composite Scoring (weighted combination of criteria):
   • Model B: 1.000
   • Model A: 0.700
   • Model C: 0.400

🎯 RECOMMENDED MODEL: Model B

Justification for Model B:
   ✓ Highest predictive power (R² = 0.8263)
   ✓ Includes important property characteristics
   ✓ Comprehensive variable coverage
   ✓ Best for prediction accuracy

Limitations of Model B:
   ⚠ More complex with potential overfitting risk
   ⚠ May have multicollinearity issues

Alternative Recommendations by Use Case:
   • For prediction accuracy: Model B
   • For model parsimony: Model B
   • For balanced approach: Model B
   • For regulatory reporting: Model A (simplest, most interpretable)
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Math6450_Assignment2_files/Math6450_Assignment2_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    9 Practical Application

Using your recommended model from Question 8:

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Predict the expected claims amount for a residential property with the
  following characteristics:

  Deductible: \$5,000

  Coverage: \$250,000

  Age: 15 years

  Prior claims: 1

  Premium: \$2,500

  Location: Urban
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Discuss the business implications of your findings. What
  recommendations would you make to an insurance company based on your
  analysis?
\end{enumerate}

    10 Critical Thinking

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  What are the key assumptions of multiple linear regression? Discuss
  whether these assumptions are likely to be satisfied in this insurance
  claims context.
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  What additional variables might be useful to include in this model to
  better predict claims amounts? Explain your reasoning.
\end{enumerate}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
